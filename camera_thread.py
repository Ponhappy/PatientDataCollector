# camera_thread.py

from PyQt5.QtCore import QThread, pyqtSignal, QMutex, QWaitCondition
import cv2
import time
from PyQt5.QtMultimedia import QSound
import os
from queue import Queue
from threading import Thread
import numpy as np
from ultralytics import YOLO
from PIL import Image
from tongue_detect.YoloModel import YOLO_model
import warnings
warnings.simplefilter("ignore", UserWarning)
from datetime import datetime
from PIL import Image

class CameraThread(QThread):
    frame_received = pyqtSignal(object)  # ä¼ é€’OpenCVå¸§
    face_detected = pyqtSignal(bool)      # ä¼ é€’äººè„¸æ£€æµ‹ç»“æœ
    # tongue_detected = pyqtSignal(bool)  # ä¼ é€’èˆŒè±¡æ£€æµ‹ç»“æœ
    guidance_message = pyqtSignal(str)    # å‘é€å¼•å¯¼æ¶ˆæ¯ä¿¡å·
    # tongue_diagnosis_ready = pyqtSignal(object)  # èˆŒå¤´è¯Šæ–­å‡†å¤‡å°±ç»ªä¿¡å·
    crop_tongue_saved_path = pyqtSignal(str)   # èˆŒå¤´è£å‰ªå›¾åƒä¿å­˜ä¿¡å·
    original_frame_saved_path = pyqtSignal(str, str)  # åŸå§‹å¸§ä¿å­˜ä¿¡å·(ä¼ é€’åŸå§‹å¸§è·¯å¾„å’Œå¯¹åº”çš„è£å‰ªå›¾è·¯å¾„)
    max_images_reached = pyqtSignal()  # å½“è¾¾åˆ°æœ€å¤§å›¾åƒæ•°æ—¶å‘å‡º
    
    # æ·»åŠ å·¥ä½œæ¨¡å¼å¸¸é‡
    MODE_PREVIEW = 0  # ä»…é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¿å­˜å›¾åƒ
    MODE_CAPTURE = 1  # æ‹æ‘„æ¨¡å¼ï¼Œå®šæœŸä¿å­˜å›¾åƒå¹¶åˆ†æ

    def __init__(self, save_dir,camera_index=0, crop_tongue_interval=5):
        super().__init__()
        self.crop_tongue_interval = crop_tongue_interval
        self.save_dir = save_dir
        self.running = True
        self.camera_index = camera_index
        self.cap = None  # å»¶è¿Ÿåˆå§‹åŒ–æ‘„åƒå¤´
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        
        # åŠ è½½èˆŒå¤´æ£€æµ‹æ¨¡å‹
        try:
            self.tongue_model = YOLO_model()
            print("èˆŒå¤´æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"èˆŒå¤´æ£€æµ‹æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.tongue_model = None
        
        # å›¾åƒå¤„ç†ç›¸å…³è®¾ç½®
        self.frame_queue = Queue(maxsize=30)  # æœ€å¤šç¼“å­˜30å¸§
        # self.processing_enabled = False  # æ˜¯å¦å¯ç”¨å¤„ç†
        # self.save_enabled = True  # æ˜¯å¦ä¿å­˜å›¾åƒ
        self.save_crop_tongue_image = True  # æ˜¯å¦ä¿å­˜è£å‰ªçš„èˆŒå¤´å›¾åƒ
        self.frames_to_skip = 10  # æ¯å¤„ç†ä¸€å¸§ï¼Œè·³è¿‡å¤šå°‘å¸§
        self.frame_count = 0
        self.processor_thread = None
        self.last_save_time = 0
        self.tongue_crop_count = 0  # ä¿å­˜çš„èˆŒå¤´è£å‰ªå›¾åƒè®¡æ•°
        self.max_tongue_crops = 10  # æœ€å¤šä¿å­˜çš„èˆŒå¤´è£å‰ªå›¾åƒæ•°é‡
        
        # èˆŒå¤´æ£€æµ‹ç›¸å…³
        self.tongue_detection_enabled = True
        self.has_tongue = False
        self.guidance_interval = 3  # æç¤ºé—´éš”ï¼Œç§’
        self.conf_threshold = 0.5  # èˆŒå¤´æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼

        # æ·»åŠ å·¥ä½œæ¨¡å¼æ§åˆ¶
        self.working_mode = self.MODE_PREVIEW  # é»˜è®¤ä¸ºé¢„è§ˆæ¨¡å¼
        self.preview_scale = 0.75  # é¢„è§ˆå›¾åƒç¼©æ”¾æ¯”ä¾‹ï¼Œå¯æé«˜æ€§èƒ½
        self.preview_interval = 0.1  # é¢„è§ˆåˆ·æ–°é—´éš”ï¼Œç§’
        self.last_preview_time = 0

        # æ·»åŠ è¯Šæ–­å®ŒæˆçŠ¶æ€æ ‡å¿—
        self.diagnosis_completed = False

        # æ·»åŠ æ ‡å¿—ä»¥è·Ÿè¸ªæ˜¯å¦å·²å‘é€ç¬¬ä¸€å¸§å›¾åƒ
        self.first_image_sent = False

        # é¢è¯Šç›¸å…³
        self.face_detection_enabled = False
        self.face_diagnosed = False
        self.face_max_images = 10
        self.face_image_count = 0
        self.face_crop_interval = 15  # æ¯15å¸§ä¿å­˜ä¸€æ¬¡

    def start_camera(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ‘„åƒå¤´"""
        if self.cap is None or not self.cap.isOpened():
            self.cap = cv2.VideoCapture(self.camera_index)
            if not self.cap.isOpened():
                print(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {self.camera_index}")
                return False
            # è®¾ç½®åˆ†è¾¨ç‡ï¼ˆå¯é€‰ï¼‰
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            return True
        return True

    def run(self):
        """ä¸»çº¿ç¨‹ï¼šåŠ¨æ€åˆ‡æ¢é¢„è§ˆå’Œæ‹æ‘„æ¨¡å¼"""
        if not self.start_camera():
            return
            
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        self.processor_thread = Thread(target=self.process_frames)
        self.processor_thread.daemon = True
        self.processor_thread.start()
        
        last_guidance_time = 0
        last_capture_time = 0
        capture_duration = 2  # æ£€æµ‹åˆ°èˆŒå¤´åæŒç»­æ‹æ‘„æ—¶é—´(ç§’)

        while self.running:
            ret, frame = self.cap.read()
            if not ret:
                continue

            current_time = time.time()
            
            # å‘å¤„ç†é˜Ÿåˆ—æ·»åŠ å›¾åƒ
            if not self.frame_queue.full():
                self.frame_queue.put(frame.copy())

            # æ¨¡å¼å¤„ç†é€»è¾‘
            if self.working_mode == self.MODE_PREVIEW:
                # é¢„è§ˆæ¨¡å¼ï¼šé™ä½å¸§ç‡æ˜¾ç¤ºç¼©æ”¾å›¾åƒ
                if current_time - self.last_preview_time >= self.preview_interval:
                    preview_frame = cv2.resize(frame, (0,0), fx=self.preview_scale, fy=self.preview_scale)
                    self.frame_received.emit(preview_frame)
                    self.last_preview_time = current_time

                # æ˜¾ç¤ºå¼•å¯¼æç¤º
                if self.tongue_detection_enabled and not self.has_tongue:
                    if current_time - last_guidance_time > self.guidance_interval:
                        self.guidance_message.emit("ğŸ‘…è¯·ä¼¸å‡ºèˆŒå¤´")
                        last_guidance_time = current_time
            

            elif self.working_mode == self.MODE_CAPTURE:
                # æ‹æ‘„æ¨¡å¼ï¼šå‘é€åŸå§‹å¸§
                self.frame_received.emit(frame)
                

            # åœ¨å¤„ç†èˆŒè¯Šä¹‹åæ·»åŠ é¢è¯Šå¤„ç†ä»£ç 
            if self.face_detection_enabled and not self.face_diagnosed:
                # ä¿å­˜ç¬¬ä¸€å¸§ä»¥è¿›è¡Œé¢è¯Š
                if self.face_image_count == 0:
                    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
                    face_image_path = os.path.join(self.save_dir, "face_images", f"face_{timestamp}.jpg")
                    cv2.imwrite(face_image_path, frame)
                    self.original_frame_saved_path.emit(face_image_path, "")  # å‘é€é¢è¯Šå›¾åƒè·¯å¾„
                    self.face_image_count += 1
                
                # ç»§ç»­ä¿å­˜å…¶ä»–9å¼ å›¾åƒä½œä¸ºå¤‡ç”¨
                elif self.frame_count % self.face_crop_interval == 0 and self.face_image_count < self.face_max_images:
                    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
                    face_image_path = os.path.join(self.save_dir, "face_images", f"face_{timestamp}.jpg")
                    cv2.imwrite(face_image_path, frame)
                    self.face_image_count += 1
                    
                    # å½“è¾¾åˆ°æœ€å¤§å›¾åƒæ•°é‡æ—¶å‘å‡ºä¿¡å·
                    if self.face_image_count >= self.face_max_images:
                        self.max_images_reached.emit()

            time.sleep(0.01)
        
        self.cap.release()

    def process_frames(self):
        """å¤„ç†çº¿ç¨‹ï¼šä»é˜Ÿåˆ—è·å–å›¾åƒå¹¶è¿›è¡Œå¤„ç†ã€ä¿å­˜"""
        print("è¿›å…¥process_frameså‡½æ•°")
        while self.running:
            if not self.frame_queue.empty():
                frame = self.frame_queue.get()
                self.frame_count += 1
                
                # åªå¤„ç†éƒ¨åˆ†å¸§ä»¥å‡å°‘è®¡ç®—é‡
                if self.frame_count % self.frames_to_skip == 0:
                    # å¦‚æœè¯Šæ–­æœªå®Œæˆä¸”å¯ç”¨äº†èˆŒå¤´æ£€æµ‹ï¼Œåˆ™æ£€æµ‹èˆŒå¤´
                    if not self.diagnosis_completed and self.tongue_detection_enabled and self.tongue_model is not None:
                        detected, bbox, confidence, crop_image = self.detect_tongue(frame)
                        self.has_tongue = detected
                        
                        # å¦‚æœæ£€æµ‹åˆ°èˆŒå¤´ä¸”æ²¡æœ‰å‘é€è¿‡ç¬¬ä¸€å¸§å›¾åƒ
                        if detected and not self.first_image_sent:
                            # åˆ‡æ¢åˆ°æ‹æ‘„æ¨¡å¼
                            self.working_mode = self.MODE_CAPTURE
                                
                            # å¦‚æœå¯ç”¨äº†ä¿å­˜ä¸”è¾¾åˆ°ä¿å­˜é—´éš”ï¼Œä¿å­˜å›¾åƒ
                            current_time = time.time()
                            if self.save_crop_tongue_image and self.tongue_crop_count < self.max_tongue_crops:
                                # ä¿å­˜è£å‰ªå›¾åƒ
                                crop_tongue_path = self.save_crop_tongue(crop_image)
                                
                                # ä¿å­˜åŸå§‹å¸§
                                original_path = self.save_original_frame(frame, crop_tongue_path)
                                
                                # æ ‡è®°å·²å‘é€ç¬¬ä¸€å¸§å›¾åƒ
                                self.first_image_sent = True
                                
                                # å‘é€ä¸¤ä¸ªè·¯å¾„ä¿¡å·
                                self.crop_tongue_saved_path.emit(crop_tongue_path)
                                self.original_frame_saved_path.emit(original_path, crop_tongue_path)
                                
                                self.last_save_time = current_time
                                self.tongue_crop_count += 1
                                
                                print("å·²å‘é€ç¬¬ä¸€å¸§å›¾åƒè¿›è¡ŒèˆŒè¯Šåˆ†æ")
                        # å¯¹åç»­æ£€æµ‹åˆ°çš„èˆŒå¤´å›¾åƒçš„å¤„ç† (åªæœ‰åœ¨æœªå‘é€ç¬¬ä¸€å¸§æ—¶æ‰å‘é€)
                        elif detected and self.first_image_sent:
                            # ç»§ç»­æ£€æµ‹ä½†ä¸å†å‘é€ä¿¡å·
                            pass
                        else:
                            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°èˆŒå¤´ï¼Œåˆ‡æ¢å›é¢„è§ˆæ¨¡å¼
                            self.working_mode = self.MODE_PREVIEW
                
            else:
                # é˜Ÿåˆ—ä¸ºç©ºæ—¶çŸ­æš‚ä¼‘çœ 
                time.sleep(0.01)

        
    def detect_tongue(self, frame):
        """èˆŒå¤´æ£€æµ‹å‡½æ•°ï¼Œä½¿ç”¨YOLOæ¨¡å‹"""
        if not self.tongue_detection_enabled:
            return False, None, 0, None
        
        try:
            # å¦‚æœè¾“å…¥æ˜¯OpenCVæ ¼å¼(BGR)ï¼Œè½¬æ¢ä¸ºRGBä»¥åŒ¹é…PILçš„æœŸæœ›æ ¼å¼
            if isinstance(frame, np.ndarray):
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # å¯é€‰ï¼šè½¬æ¢ä¸ºPILå›¾åƒä»¥ç¡®ä¿æ ¼å¼å®Œå…¨åŒ¹é…
                # frame_pil = Image.fromarray(frame_rgb)
                detected, bbox, conf, crop_img = self.tongue_model.detect_single_image(frame_rgb,crop=True)
            else:
                # å¦‚æœå·²ç»æ˜¯PILæ ¼å¼ï¼Œç›´æ¥ä¼ å…¥
                detected, bbox, conf, crop_img = self.tongue_model.detect_single_image(frame, crop=True)
            
            print(f"è½½å…¥detect_tongueå‡½æ•°ï¼Œæ£€æµ‹ç»“æœä¸º{detected}")
            return detected, bbox, conf, crop_img
        except Exception as e:
            print(f"èˆŒå¤´æ£€æµ‹å‡ºé”™: {str(e)}")
            return False, None, 0, None

    def save_crop_tongue(self, crop_image):
        """ä¿å­˜è£å‰ªçš„èˆŒå¤´å›¾åƒå¹¶è¿”å›è·¯å¾„"""
        user_dir = os.path.join(self.save_dir, "tongue_crops")
        os.makedirs(user_dir, exist_ok=True)
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        crop_path = os.path.join(user_dir, f"crop_{timestamp}.jpg")
        # åˆ¤æ–­å›¾åƒç±»å‹å¹¶ä¿å­˜
        if isinstance(crop_image, np.ndarray):  # OpenCVæ ¼å¼
            cv2.imwrite(crop_path, crop_image)
            # cv2.imwrite(crop_path, crop_image, [cv2.IMWRITE_JPEG_QUALITY, 95])
        elif hasattr(crop_image, 'save'):      # PILæ ¼å¼
            crop_image.save(crop_path)
            # crop_image.save(crop_path, format='JPEG', quality=95)
        else:
            print(f"æ— æ³•è¯†åˆ«çš„å›¾åƒç±»å‹: {type(crop_image)}")
            return None
        
        
        
        return crop_path
    
    def set_tongue_detection_enabled(self, enabled):
        """è®¾ç½®æ˜¯å¦å¯ç”¨èˆŒå¤´æ£€æµ‹"""
        self.tongue_detection_enabled = enabled
        
    def set_save_crop_tongue_enabled(self, enabled):
        """è®¾ç½®æ˜¯å¦ä¿å­˜è£å‰ªçš„èˆŒå¤´å›¾åƒ"""
        self.save_crop_tongue_image = enabled
        
    def set_frames_to_skip(self, count):
        """è®¾ç½®è·³è¿‡çš„å¸§æ•°"""
        if count > 0:
            self.frames_to_skip = count
            
    def set_crop_tongue_interval(self, interval):
        """è®¾ç½®æˆªå›¾é—´éš”ï¼ˆç§’ï¼‰"""
        if interval > 0:
            self.crop_tongue_interval = interval

    def set_mode(self, mode):
        """è®¾ç½®å·¥ä½œæ¨¡å¼ï¼šé¢„è§ˆæˆ–æ‹æ‘„"""
        self.working_mode = mode
        print(f"æ‘„åƒå¤´å·¥ä½œæ¨¡å¼å·²åˆ‡æ¢: {'é¢„è§ˆ' if mode == self.MODE_PREVIEW else 'æ‹æ‘„'}")
        
    def pause(self):
        """æš‚åœçº¿ç¨‹è¿è¡Œ"""
        self.running = False
        # é‡Šæ”¾æ‘„åƒå¤´èµ„æº
        if self.cap and self.cap.isOpened():
            self.cap.release()
            self.cap = None
        print("æ‘„åƒå¤´çº¿ç¨‹å·²æš‚åœ")

    def resume(self):
        """æ¢å¤çº¿ç¨‹è¿è¡Œ"""
        if self.isRunning():
            print("æ‘„åƒå¤´çº¿ç¨‹å·²ç»åœ¨è¿è¡Œ")
            return  # å¦‚æœçº¿ç¨‹å·²ç»åœ¨è¿è¡Œï¼Œä¸åšä»»ä½•äº‹
        
        self.running = True
        self.cap = None  # ç¡®ä¿æ‘„åƒå¤´è¢«é‡æ–°åˆå§‹åŒ–
        self.start()  # é‡æ–°å¯åŠ¨çº¿ç¨‹
        print("æ‘„åƒå¤´çº¿ç¨‹å·²æ¢å¤")

    def stop(self):
        """åœæ­¢çº¿ç¨‹"""
        self.running = False
        if self.processor_thread:
            self.processor_thread.join(timeout=1.0)
        self.wait()

    def set_diagnosis_completed(self, completed):
        """è®¾ç½®èˆŒè¯Šæ˜¯å¦å·²å®Œæˆ"""
        self.diagnosis_completed = completed
        if completed:
            print("èˆŒè¯Šå·²å®Œæˆï¼ŒèˆŒå¤´æ£€æµ‹å°†åœæ­¢")
            self.tongue_detection_enabled = False  # è‡ªåŠ¨ç¦ç”¨èˆŒå¤´æ£€æµ‹
        else:
            # å¦‚æœé‡æ–°å¼€å§‹è¯Šæ–­ï¼Œé‡ç½®å‘é€å›¾åƒæ ‡å¿—
            self.first_image_sent = False

    def save_original_frame(self, frame, crop_image_path):
        """ä¿å­˜åŒ…å«èˆŒå¤´çš„åŸå§‹å¸§"""
        # ä»è£å‰ªå›¾è·¯å¾„æå–åŸºæœ¬åç§°
        base_dir = os.path.dirname(crop_image_path)
        base_name = os.path.basename(crop_image_path)
        name_without_ext = os.path.splitext(base_name)[0]
        
        # åˆ›å»ºåŸå§‹å¸§çš„æ–‡ä»¶å
        original_path = os.path.join(base_dir, f"{name_without_ext}_original.jpg")
        
        # ä¿å­˜åŸå§‹å¸§
        cv2.imwrite(original_path, frame)
        print(f"åŸå§‹å¸§ä¿å­˜åˆ°: {original_path}")
        
        return original_path

    def set_face_detection_enabled(self, enabled):
        """å¯ç”¨æˆ–ç¦ç”¨é¢è¯Šæ£€æµ‹"""
        self.face_detection_enabled = enabled
        if enabled:
            self.face_diagnosed = False
            self.face_image_count = 0
            print("é¢è¯Šæ£€æµ‹å·²å¯ç”¨")